---
title: "Detecting Duplicated Patient Encounters by Visit_ID, Medical_Record_Number, and Facility"
author: ""
date: "`r format(Sys.time(), '%B %-d, %Y')`"
output: html_document
---

```{=html}
<style type="text/css">
  .main-container {
    max-width: 1500px;
    margin-left: auto;
    margin-right: auto;
  }
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	fig.align = "center",
	dplyr.summarise.inform = FALSE,
	echo = FALSE,
	knitr.figure = TRUE,
	message = FALSE,
	warning = FALSE,
	error = FALSE
	)

options(scipen = 999)

pacman::p_load(tidyverse, tidycensus, sjmisc, grid, gridExtra, kableExtra, cowplot, viridisLite, patchwork, Rnssp)
```

```{r set_end_start_dates, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
endDate <- format(Sys.Date() - 4, "%d%b%Y")
startDate <- format(Sys.Date() - 64, "%d%b%Y")
```

```{r set facilities or counties, eval=FALSE, include=FALSE}
fac <- ""
counties <- ""
```

```{r set_nssp_user_profile, echo=FALSE, message=FALSE, include=FALSE}
## Set NSSP user profile
myProfile <- readRDS("myProfile.rds")
```

Patient encounters in ESSENCE can be duplicated due to a combination of common hospital administrative practices and BioSense's message processing methodology.

Patient encounters can be duplicated for several reasons. For example:

-   Patient encounters can be duplicated due to mid-encounter changes or updates to the patient encounter's initially assigned Admit_Date_Time/C_Visit_Date_Time (e.g., patient arrives at 10PM, update message is sent at 2 AM the following calendar day with a later Admit_Date_Time than originally reported).

-   Duplication can also be caused by facility-side data quality issues, such as mid-encounter changes or updates to a patient encounter's assigned assigned medical record number (MRN) (primarily occurs due to hospital or hospital department transfer).

-   BioSense processing rules can cause duplication across a single Visit_ID whenever there is any change to a patient encounter's C_Biosense_ID, as that is the field used to assess for unique patient visits.

BioSense does not have build-in procedures in place to control for these types of data quality issues, so sites should try to identify and address these sorts of data quality issues with the offending facility or facilities directly.

Depending on the manner of duplication present, the proportion of duplicated encounters present can be affected by the average or typical encounter presentation time for that syndrome definition. For syndrome definitions (either CC/DD category definitions or custom definitions) designed to identify encounters that are influenced by behavioral habits (e.g., drug overdoses tend to occur later in the day than ED visits for chronic conditions), the likelihood and thus, the impact of duplicated patient encounters can be greater due to the later mean patient arrival time/time from Admit_Date_Time.

The same point holds for syndrome definitions that assess for encounters where a facility transfer may be more likely to occur (e.g., likelihood of surgery requiring transfer to a higher level trauma center is higher for some syndromes opposed to others), which can also influence perceived community burden.

Reviewing your data at the line level is really the only way to identify and detect these issues. Patient demographic fields, reported chief complaints, assigned discharge diagnosis codes, Visit_ID, Medical_Record_Number, and Admit_Date_Time/C_Visit_Date_Time can all assist with identifying duplicated encounters.

This practice may not be necessary for all sites, and site-specific data quality metrics and/or data submission procedures may also affect the burden of duplicated encounters. I would recommend checking your site's data for this issue if you intend to utilize a syndrome for anomaly detection at the county level or below.

```{r datadetails pull, fig.width=12}
category_list <- "air%20quality-related%20respiratory%20illness%20v1"

url <- paste0("https://essence.syndromicsurveillance.org/nssp_essence/api/dataDetails?datasource=va_hosp&startDate=",startDate, "&medicalGroupingSystem=essencesyndromes&userId=2765&endDate=",endDate,"&percentParam=noPercent&site=895&hospFacilityType=emergency%20care&aqtTarget=DataDetails&ccddCategory=",category_list,"&geographySystem=hospital&detector=probrepswitch&timeResolution=daily&hasBeenE=1")

api_data <- myProfile$get_api_data(url) %>%
  pluck("dataDetails")

# De-duplicate by Visit_ID (this is the method I'd suggest using to deduplicate data details API pulls)
# Practically speaking, you can either use HospitalName (facility name from your site's MFT) or Hospital (C_Biosense_Facility_ID)
# This method will filter to the first row for each Visit_ID, thus removing the "duplicated" encounter(s) represented by the second or third row for a given Visit_ID when present
air_quality_deduped <- api_data %>% 
  group_by(Hospital, Visit_ID) %>% 
  filter(row_number() == 1) %>% 
  ungroup()

# Examine Duplicates by Hospital Received
air_quality_dupes <- api_data %>% 
  select(Date, Time, C_Visit_Date_Time, HospitalName, ZipCode, Region, Sex, C_Patient_Age, Ethnicity = c_ethnicity, Race = c_race, Medical_Record_Number, Visit_ID, CCDDParsed) %>% 
  group_by(HospitalName, Visit_ID) %>% 
  summarize(n = n()) %>% 
  ungroup() %>% 
  filter(n > 1)

# Examine duplicates at line level
air_quality_dupes_linelevel <- api_data %>% 
  dplyr::select(Date, Time, C_Visit_Date_Time, HospitalName, ZipCode, Region, Sex, C_Patient_Age, Ethnicity = c_ethnicity, Race = c_race, Medical_Record_Number, Visit_ID, CCDDParsed) %>% 
  group_by(HospitalName, Visit_ID) %>% 
  filter(row_number() > 1) %>% 
  ungroup() %>% 
  group_by(Visit_ID) %>% 
  arrange()

# Pull duplicates not due to MRN change (when multiple rows per Visit_ID exist, but Medical_Record_Number field remains the same across both "encounters")
air_quality_dupes_noMRNchange <- api_data %>% 
  dplyr::select(Date, Time, C_Visit_Date_Time, HospitalName, ZipCode, Region, Sex, C_Patient_Age, Ethnicity = c_ethnicity, Race = c_race, Medical_Record_Number, Visit_ID, CCDDParsed) %>% 
  group_by(HospitalName, Visit_ID, Medical_Record_Number) %>%
  summarize(n = n()) %>% 
  ungroup() %>% 
  filter(n > 1)

# Pull Visit_IDs with more than one row per ID
Visit_ID.dupes <- api_data %>% 
  group_by(HospitalName, Visit_ID) %>% 
  summarize(n = n()) %>% 
  filter(n > 1) %>% 
  ungroup() %>% 
  select(Visit_ID) %>% 
  pull()

# Pull duplicated encounters by Visit_ID from vector created by Visit_ID.dupes (duplicated for any reason)
duplicated_air_quality_visits <- api_data %>% 
  filter(Visit_ID %in% Visit_ID.dupes) %>% 
  group_by(Hospital, Visit_ID) %>% 
  arrange(Visit_ID) %>% 
  ungroup()

# kbl to examine duplicated encounters
duplicated_air_quality_visits %>% 
    select(Date, Time, C_Visit_Date_Time, HospitalName, ZipCode, Region, Sex, C_Patient_Age, Ethnicity = c_ethnicity, Race = c_race, Medical_Record_Number, Visit_ID, ChiefComplaintParsed, CCDDParsed) %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  scroll_box(width = "100%", height = "500px")
```

## Plots from Example Data

The difference in smoothed trends isn't particularly apparent at the state level, but is readily apparent at the county level (not shown).

```{r plots, fig.width=12, fig.height=5}

airquality.withdupes <- api_data %>% 
  mutate(Date = mdy(Date)) %>% 
  group_by(Date) %>% 
  summarize(n = n()) %>% 
  ungroup()

dupe.total <- api_data %>% 
  count()

p1 <- ggplot(airquality.withdupes) +
  geom_line(aes(Date, n, color = "CDC Air Quality-Related v1 Encounters")) +
  geom_smooth(
    aes(Date, n, color = "Smoothed Trend"),
    se = FALSE,
    method = 'loess'
  ) +
  labs(x = "Date",
       y = "Patient Encounters (n)",
       title = "CDC Air Quality-Related v1 Encounters (ED Visits Only)\nIncluding Duplicated Encounters",
       subtitle = paste0("n = ", dupe.total)) +
  geom_point(aes(Date, n, color = "CDC Air Quality-Related v1 Encounters"),
             size = 1.25) +
  scale_x_date(date_labels = "%b %d",
               expand = c(0, 0.1)) +
  scale_y_continuous(
    breaks = scales::pretty_breaks(8),
    limits = c(0, NA)
  ) +
  scale_color_manual(values = c("CDC Air Quality-Related v1 Encounters" = "Blue",
                                "Smoothed Trend" = "red")) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 12),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.title = element_blank(),
    legend.position = "bottom"
  )
  
deduped <- air_quality_deduped %>% 
  mutate(Date = mdy(Date)) %>% 
  group_by(Date) %>% 
  summarize(n = n()) %>% 
  ungroup()

nodupe.total <- air_quality_deduped %>% 
  count()

p2 <- ggplot(deduped) +
  geom_line(aes(Date, n, color = "CDC Air Quality-Related v1 Encounters\n(no duplicated encounters)")) +
  geom_smooth(
    aes(Date, n, color = "Smoothed Trend"),
    se = FALSE,
    method = 'loess'
  ) +
  labs(x = "Date",
       y = "Patient Encounters (n)",
       title = "CDC Air Quality-Related v1 Encounters (ED Visits Only)\nDuplicated Encounters Removed",
       subtitle = paste0("n = ", nodupe.total)) +
  geom_point(aes(Date, n, color = "CDC Air Quality-Related v1 Encounters\n(no duplicated encounters)"),
             size = 1.25) +
  scale_x_date(date_labels = "%b %d",
               expand = c(0, 0.1)) +
  scale_y_continuous(
    breaks = scales::pretty_breaks(8),
    limits = c(0, NA)
  ) +
  scale_color_manual(values = c("CDC Air Quality-Related v1 Encounters\n(no duplicated encounters)" = "Red",
                                "Smoothed Trend" = "blue")) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 12),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.title = element_blank(),
    legend.position = "bottom"
  )

p1 + p2
```
